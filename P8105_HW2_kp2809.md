P8105_HW2_kp2809
================
Kimberly Lopez
2024-09-24

# Problem 1

Read and clean the data retain line, station, name, station latitude /
longitude, routes served, entry, vending, entrance type, and ADA
compliance.

``` r
transit_df= 
  read_csv("NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
  na= c("NA","",".") 
  )
```

    ## Rows: 1868 Columns: 32
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
    ## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
    ## lgl  (2): ADA, Free Crossover
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
transit_tidy_df= 
  transit_df|>
  janitor::clean_names() 
  
select(transit_tidy_df, line:ada, -(staffing),-(staff_hours), -(exit_only) )
```

    ## # A tibble: 1,868 × 19
    ##    line     station_name station_latitude station_longitude route1 route2 route3
    ##    <chr>    <chr>                   <dbl>             <dbl> <chr>  <chr>  <chr> 
    ##  1 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ##  2 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ##  3 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  4 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  5 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  6 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  7 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  8 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  9 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ## 10 4 Avenue 53rd St                  40.6             -74.0 R      <NA>   <NA>  
    ## # ℹ 1,858 more rows
    ## # ℹ 12 more variables: route4 <chr>, route5 <chr>, route6 <chr>, route7 <chr>,
    ## #   route8 <dbl>, route9 <dbl>, route10 <dbl>, route11 <dbl>,
    ## #   entrance_type <chr>, entry <chr>, vending <chr>, ada <lgl>

``` r
transit_tidy_df= 
  transit_df|>
  janitor::clean_names() |>
  select( line:ada, -(staffing),-(staff_hours), -(exit_only) )
```

Convert the entry variable from character (YES vs NO) to a logical
variable (the ifelse or case_match function may be useful).

``` r
transit_tidy_df= 
  transit_df|>
  janitor::clean_names() |>
  select( line:ada, -(staffing),-(staff_hours), -(exit_only) ) |>
  mutate(
  entry = ifelse(entry =="YES", TRUE,FALSE))
```

Write a short paragraph about this dataset – explain briefly what
variables the dataset contains, describe your data cleaning steps so
far, and give the dimension (rows x columns) of the resulting dataset.
Are these data tidy?

This data set provides data on transit routes, lines, and some spatial
data. I have cleaned the names of some of these variables using the
janitor function. I have also labeled missing values. I also changed the
responses in the entry column to a logical vector of TRUE = “yes” and
FALSE = “no” entry. After selecting certain columns, there are 1858 rows
and 19 columns. This is not yet tidy data. Routes are separated into
more than one column and is not easy to follow. Aside from that, the
variable names are tidy.

How many distinct stations are there? Note that stations are identified
both by name and by line (e.g. 125th St 8th Avenue; 125st Broadway;
125st Lenox); the distinct function may be useful here.

``` r
transit_tidy_df |> 
  select(line, station_name)|>
  distinct()
```

    ## # A tibble: 465 × 2
    ##    line     station_name            
    ##    <chr>    <chr>                   
    ##  1 4 Avenue 25th St                 
    ##  2 4 Avenue 36th St                 
    ##  3 4 Avenue 45th St                 
    ##  4 4 Avenue 53rd St                 
    ##  5 4 Avenue 59th St                 
    ##  6 4 Avenue 77th St                 
    ##  7 4 Avenue 86th St                 
    ##  8 4 Avenue 95th St                 
    ##  9 4 Avenue 9th St                  
    ## 10 4 Avenue Atlantic Av-Barclays Ctr
    ## # ℹ 455 more rows

There are 465 distinct station names considering the two identifiers of
station name and line.

How many stations are ADA compliant?

``` r
transit_tidy_df |>
  filter(ada== TRUE) |>
  select(station_name, line)|>
  distinct()
```

    ## # A tibble: 84 × 2
    ##    station_name                   line           
    ##    <chr>                          <chr>          
    ##  1 Atlantic Av-Barclays Ctr       4 Avenue       
    ##  2 DeKalb Av                      4 Avenue       
    ##  3 Pacific St                     4 Avenue       
    ##  4 Grand Central                  42nd St Shuttle
    ##  5 34th St                        6 Avenue       
    ##  6 47-50th Sts Rockefeller Center 6 Avenue       
    ##  7 Church Av                      6 Avenue       
    ##  8 21st St                        63rd Street    
    ##  9 Lexington Av                   63rd Street    
    ## 10 Roosevelt Island               63rd Street    
    ## # ℹ 74 more rows

The tibble I filtered for ada compliance equal to “TRUE” returned 468
rows suggesting 468 stations are ada compliant, but I needed to remove
duplicate stations. There are 84 distinct stations which are ada
compliant.

What proportion of station entrances or exits without vending allow
entrance?

- station entry = true and vending = NO (entry = “YES” )
- and station exit and vending = NO

``` r
transit_tidy_df|>
  filter(vending== "NO") |>
  pull(entry) |>
  mean()
```

    ## [1] 0.3770492

The proportion of station entrances or exits without vending which allow
entrance was 0.38.

Reformat data so that route number and route name are distinct
variables. - merge all route# columns

``` r
transit_tidy_route =
  transit_tidy_df |> 
  mutate(across(starts_with("route"), as.character))|>
  pivot_longer(
    cols= route1:route11, 
    names_to = "route_number", 
    names_prefix = "route",
    values_to = "route"
  )|>
  relocate(
    route_number, route,station_name
  )
```

How many distinct stations serve the A train?

``` r
transit_tidy_route |>
  filter(route =="A") |> 
  distinct(station_name, line)
```

    ## # A tibble: 60 × 2
    ##    station_name                  line           
    ##    <chr>                         <chr>          
    ##  1 Times Square                  42nd St Shuttle
    ##  2 125th St                      8 Avenue       
    ##  3 145th St                      8 Avenue       
    ##  4 14th St                       8 Avenue       
    ##  5 168th St - Washington Heights 8 Avenue       
    ##  6 175th St                      8 Avenue       
    ##  7 181st St                      8 Avenue       
    ##  8 190th St                      8 Avenue       
    ##  9 34th St                       8 Avenue       
    ## 10 42nd St                       8 Avenue       
    ## # ℹ 50 more rows

There are 60 distinct station the A line serves.

Of the stations that serve the A train, how many are ADA compliant?

- ada compliant stations divided by distinct() stations that serve the
  == A train

``` r
transit_tidy_route |> 
  filter (route == "A" , ada== TRUE) |> 
  distinct(station_name, line)
```

    ## # A tibble: 17 × 2
    ##    station_name                  line            
    ##    <chr>                         <chr>           
    ##  1 14th St                       8 Avenue        
    ##  2 168th St - Washington Heights 8 Avenue        
    ##  3 175th St                      8 Avenue        
    ##  4 34th St                       8 Avenue        
    ##  5 42nd St                       8 Avenue        
    ##  6 59th St                       8 Avenue        
    ##  7 Inwood - 207th St             8 Avenue        
    ##  8 West 4th St                   8 Avenue        
    ##  9 World Trade Center            8 Avenue        
    ## 10 Times Square-42nd St          Broadway        
    ## 11 59th St-Columbus Circle       Broadway-7th Ave
    ## 12 Times Square                  Broadway-7th Ave
    ## 13 8th Av                        Canarsie        
    ## 14 Franklin Av                   Franklin        
    ## 15 Euclid Av                     Fulton          
    ## 16 Franklin Av                   Fulton          
    ## 17 Howard Beach                  Rockaway

Of those stations that serve the A train, there are 17 stations that are
ada compliant.

## Problem 2

Read and clean the Mr. Trash Wheel excel sheet:

- specify the sheet in the Excel file and to omit non-data entries (rows
  with notes / figures; columns containing notes) using arguments in
  read_excel
- use reasonable variable names
- omit rows that do not include dumpster-specific data
- round the number of sports balls to the nearest integer and converts
  the result to an integer variable (using as.integer)

``` r
trash_df = 
  read_excel("202409 Trash Wheel Collection Data.xlsx", 
    sheet = "Mr. Trash Wheel",
    range = "A2:N586") |>
  janitor::clean_names()|>
  mutate(
    sports_balls= round(sports_balls,digit=1), 
    sports_balls = as.integer(sports_balls)
  )
```

Use a similar process to import, clean, and organize the data for
Professor Trash Wheel and Gwynnda, and combine this with the Mr. Trash
Wheel dataset to produce a single tidy dataset. To keep track of which
Trash Wheel is which, you may need to add an additional variable to both
datasets before combining.

Imported both datasets and added the same column which specifies which
wheel the data is coming from.

``` r
gwynnda_df = 
  read_excel("202409 Trash Wheel Collection Data.xlsx", 
    sheet = "Gwynnda Trash Wheel", 
    range = "A2:L157")|>
  janitor::clean_names()|>
  mutate(
    wheel= "gwynnda"
  )

professor_df = 
  read_excel("202409 Trash Wheel Collection Data.xlsx", 
    sheet = "Professor Trash Wheel", 
    range = "A2:M106") |>
  janitor::clean_names()|>
  mutate(
    wheel= "professor"
  )
trash_df = 
  read_excel("202409 Trash Wheel Collection Data.xlsx", 
    sheet = "Mr. Trash Wheel",
    range = "A2:N586") |>
  janitor::clean_names()|>
  mutate(
    sports_balls= round(sports_balls,digit=1), 
    sports_balls = as.integer(sports_balls), 
    wheel= "mr", 
    year= as.numeric(year)
  )
```

Merge the 3 datasets

- merge using the stack bind() method

``` r
wheel_df = 
  bind_rows(trash_df,gwynnda_df,professor_df)
```

Write a paragraph about these data; you are encouraged to use inline R.
Be sure to note the number of observations in the resulting dataset, and
give examples of key variables. For available data, what was the total
weight of trash collected by Professor Trash Wheel? What was the total
number of cigarette butts collected by Gwynnda in June of 2022?

``` r
wheel_df |>
  filter(wheel == "professor") |>
  summarize(
     sum = sum(weight_tons)
  )
```

    ## # A tibble: 1 × 1
    ##     sum
    ##   <dbl>
    ## 1  212.

``` r
wheel_df |>
  filter(month == "June", wheel == "gwynnda", year== "2022") |>
  summarize(
     sum = sum(cigarette_butts)
  )
```

    ## # A tibble: 1 × 1
    ##     sum
    ##   <dbl>
    ## 1 18120

There are 843 rows in the wheel_df which holds row entries from gwynnda
155 rows, professor 104 rows, and mr. wheel datasets 584 rows. Each row
observation still shows observations from a date taken at a moment in
time (year). Some key variables are dumpster, month, year, date,
weight_tons, volume_cubic_yards, plastic_bottles, polystyrene,
cigarette_butts, glass_bottles, plastic_bags, wrappers, sports_balls,
homes_powered, wheel where “wheel” describes if the observations are
from Mr.Trash wheel, Professor Trash Wheel, or Gwynnda Trash Wheel. The
total weight of trash collected by Professor Trash Wheel was 212.16
tons. 18,120 cigarette butts in June 2022 collected by Gwynnda Trash
Wheel.

## Problem 3

In the first part of this problem, your goal is to create a single,
well-organized dataset with all the information contained in these data
files.

- import, clean, tidy, and otherwise wrangle each of these dataset

- check for completeness and correctness across datasets (e.g. by
  viewing individual datasets and using anti_join)

- merge to create a single, final dataset; and organize this so that
  variables and observations are in meaningful orders.

- Export the result as a CSV in the directory containing the original
  datasets.

``` r
bakers_df = 
  read_csv("gbb_datasets/bakers.csv", na= c("NA","",".")) |> 
  janitor:: clean_names() |>
  separate(
    baker_name, into = c("baker","last_name"), sep=" "
    ) |> 
  mutate(baker= str_to_lower(baker))
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
bakes_df = 
  read_csv("gbb_datasets/bakes.csv", na= c("NA","",".")) |> 
  janitor:: clean_names() |> 
  mutate(baker= str_to_lower(baker))
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
results_df = 
  read_csv("gbb_datasets/results.csv", na= c("NA","","."),
           skip = 2) |> 
  janitor:: clean_names() |> 
  mutate(baker = str_to_lower(baker))
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Joining the first two datasets which have 3 matching identifiers.

``` r
bakers_series = 
  full_join( results_df, bakes_df, by = c("baker", "episode", "series")) |> 
  relocate( baker, series, episode)
```

Joining the next dataset which only has 2 matching identifiers.

``` r
bakers_full= 
  full_join (bakers_df,bakers_series, by = c("baker","series")) |>
  mutate(
    first_name = baker, 
    last_name = str_to_lower(last_name)
  )|> 
  relocate(first_name, last_name, series, episode)
```

``` r
without_match_join2 = 
  anti_join(bakers_df,bakers_series, by = c("baker","series"))

without_match_join1 = 
  anti_join(results_df, bakes_df, by = c("baker", "episode", "series"))
```

``` r
bakers_full |>
  distinct(first_name, last_name)
```

    ## # A tibble: 122 × 2
    ##    first_name last_name  
    ##    <chr>      <chr>      
    ##  1 ali        imdad      
    ##  2 alice      fevronia   
    ##  3 alvin      magallanes 
    ##  4 amelia     lebruin    
    ##  5 andrew     smyth      
    ##  6 annetha    mills      
    ##  7 antony     amourdoux  
    ##  8 beca       lyne-pirkis
    ##  9 ben        frazer     
    ## 10 benjamina  ebuehi     
    ## # ℹ 112 more rows

``` r
bakers_df |>
  distinct(baker, last_name)
```

    ## # A tibble: 120 × 2
    ##    baker     last_name  
    ##    <chr>     <chr>      
    ##  1 ali       Imdad      
    ##  2 alice     Fevronia   
    ##  3 alvin     Magallanes 
    ##  4 amelia    LeBruin    
    ##  5 andrew    Smyth      
    ##  6 annetha   Mills      
    ##  7 antony    Amourdoux  
    ##  8 beca      Lyne-Pirkis
    ##  9 ben       Frazer     
    ## 10 benjamina Ebuehi     
    ## # ℹ 110 more rows

After anti_joining some of these joinings for rows where x and y had no
matches, I doubled checked by visually viewing some of these
without-matches to ensure they are kept in the final dataset as long as
they do not repeat an exisiting match with the same idenitfiers. I also
ensured that the distinct names of bakers were also all present in the
final dataset using the distinct function. I noticed there are 2 extra
distinct names that was accumulated from the other datasets not included
in the bakers_df. This could be due to a data entry error of who “jo” is
since someone likely used a nick-name instead of their first and last
name which requires further questioning.

I imported the datasets accounting for any missing data. I skipped 2
lines for the “results_df” since the first two rows were not column
names. I cleaned the column names by using janitor and clean names
functions. I also separated the bakers names from the “bakers_df” as it
had both first and last names. This made it easier to combine these
datasets by two identifiers series, episode, and name. I combined
bakers_df by only bakers first name and series as it did not have a
episode column. Since each dataset measured different observations in
columns, I joined the datasets. I used full join to ensure that no
episodes were lost from left or right joining when joining results_df
and bakes_df. I then joined the bakers_df by baker and series to the
dataset since it had no episode column. The final dataset has 1145 rows
and 12 columns which include first_name, last_name, series, episode,
baker, baker_age, baker_occupation, hometown, technical, result,
signature_bake, show_stopper.

Create a reader-friendly table showing the star baker or winner of each
episode in Seasons 5 through 10. Comment on this table – were there any
predictable overall winners? Any surprises? - use knitr function

``` r
star_baker_df = 
  bakers_full |>
  filter(result== "STAR BAKER",series > 5)|>
  distinct(first_name, last_name, .keep_all=TRUE)|>
  relocate(first_name, last_name, series)|>
  select(first_name:episode, baker_age:technical, signature_bake, show_stopper)|>
  mutate(
    first_name= str_to_title(first_name),
    last_name = str_to_title(last_name)
  )|>
  knitr::kable(align = 'c')

star_baker_df
```

| first_name | last_name | series | episode | baker_age | baker_occupation | hometown | technical | signature_bake | show_stopper |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| Alice | Fevronia | 10 | 2 | 28 | Geography teacher | Essex | 1 | NA | NA |
| Andrew | Smyth | 7 | 7 | 25 | Aerospace engineer | Derby / Holywood, County Down | 1 | Tropical Holiday Roulade | Childhood Ice Cream Mousse Cakes |
| Benjamina | Ebuehi | 7 | 4 | 23 | Teaching assistant | South London | 1 | Red Onion Chutney, Brie and Bacon Yorkshire Puddings | Tropical Churros |
| Briony | Williams | 9 | 6 | 33 | Full-time parent | Bristol | 1 | NA | NA |
| Candice | Brown | 7 | 2 | 31 | PE teacher | Barton-Le-Clay, Bedfordshire | 8 | Salted Caramel, Chocolate Iced Shiny Hearts | Gingerbread Pub with Sticky Ginger Carpet |
| Dan | Beasley-Harling | 9 | 4 | 36 | Full-time parent | London | 1 | NA | NA |
| Henry | Bird | 10 | 7 | 20 | Student | Durham | 3 | NA | NA |
| Ian | Cumming | 6 | 2 | 41 | Travel photographer | Great Wilbraham, Cambridgeshire | 3 | Orange, Rosemary and Almond Biscotti | Sandwich de la Confiture |
| Jane | Beedle | 7 | 1 | 61 | Garden designer | Beckenham | 7 | Lemon and Poppy Seed Drizzle Cake | Chocolate Orange Mirror Cake |
| Julia | Chernogorova | 8 | 3 | 21 | Aviation Broker | Crawley, West Sussex | 2 | Earl Grey Dried Fruit Teacakes | ‘The Snail Under a Mushroom’ Bread Sculpture |
| Kate | Lyon | 8 | 4 | 29 | Health and safety inspector | Merseyside | 6 | Salted Bay Caramel Millionaire Shortbreads | Sticky Toffee Apple Caramel Cake |
| Kim-Joy | Hewlett | 9 | 5 | 27 | Mental health specialist | Leeds | 4 | NA | NA |
| Liam | Charles | 8 | 6 | 19 | Student | North London | 4 | ‘Standard FC’ Decorative Pies | ‘Nan’s Sunday Dinner’ Pie |
| Manon | Lagrave | 9 | 1 | 26 | Software project manager | London | 3 | NA | NA |
| Marie | Campbell | 6 | 1 | 66 | Retired | Auchterarder, Perthshire | 3 | Zingy Citrus Madeira Cake | A Walk in the Black Forest |
| Mat | Riley | 6 | 6 | 37 | Fire fighter | London | 1 | Piña Colada Frangipane Tart | His ‘n’ Hers Vol-au-vents |
| Michael | Chakraverty | 10 | 3 | 26 | Theatre manager/fitness instructor | Stratford-upon-Avon | 6 | NA | NA |
| Michelle | Evans-Fecci | 10 | 1 | 35 | Print shop administrator | Tenby, Wales | 6 | NA | NA |
| Nadiya | Hussain | 6 | 5 | 30 | Full-time mother | Leeds / Luton | 1 | Naked Blueberry and Caraway Crunch Cake | Chocolate and Strawberry Lime Ice Cream Roll |
| Rahul | Mandal | 9 | 2 | 30 | Research scientist | Rotherham | 2 | NA | NA |
| Ruby | Bhogal | 9 | 8 | 29 | Project manager | London | 3 | NA | NA |
| Sophie | Faldo | 8 | 5 | 33 | Former army officer and trainee stuntwoman | West Molesey, Surrey | 1 | Ginger, Fig and Honey Steamed School Pudding | Raspberry, Yuzu & White Chocolate Bûche Trifle Terrine |
| Stacey | Hart | 8 | 8 | 42 | Former school teacher | Radlett, Hertfordshire | 3 | Camembert & Onion and Apple & Blueberry Bedfordshire Clangers | ‘Bright’ Lemon & Orange Savoy Cake |
| Steph | Blackwell | 10 | 4 | 28 | Shop assistant | Chester | 1 | NA | NA |
| Steven | Carter-Bailey | 8 | 1 | 34 | Marketer | Watford, Hertfordshire | 6 | Bonfire Night Cake | ‘A Baker’s Lunch’ Cake |
| Tamal | Ray | 6 | 7 | 29 | Trainee anaesthetist | Manchester | 3 | Middle Eastern Game Pie | Spiced Blackberry, Raspberry and Cardamom Charlotte Russe |
| Tom | Gilliford | 7 | 3 | 26 | Project engagement manager | Rochdale | 4 | Chocolate Orange and Chilli Swirl Bread | Jörmungandr and Mjölnir |

I was surprised to see a young star baker or winner be as young as 19
sine the majority of winners are older adults. The amount of technical a
person had seemed to show if they would be winners. However, two
individuals seemed to have had the highest technical and still won in a
series.

Import, clean, tidy, and organize the viewership data in viewers.csv.
Show the first 10 rows of this dataset. What was the average viewership
in Season 1? In Season 5?

``` r
viewers_df = 
  read_csv("gbb_datasets/viewers.csv", na= c("NA","",".")) |> 
  janitor:: clean_names()
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
