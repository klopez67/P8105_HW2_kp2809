---
title: "P8105_HW2_kp2809"
author: "Kimberly Lopez"
date: "2024-09-24"
output: github_document
---
```{r include=FALSE}
library(tidyverse)
library(readxl)

```


# Problem 1 

Read and clean the data retain line, station, name, station latitude / longitude, routes served, entry, vending, entrance type, and ADA compliance.

```{r}

transit_df= 
  read_csv("NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
  na= c("NA","",".") 
  )

transit_tidy_df= 
  transit_df|>
  janitor::clean_names() 
  
select(transit_tidy_df, line:ada, -(staffing),-(staff_hours), -(exit_only) )
```

```{r}
transit_tidy_df= 
  transit_df|>
  janitor::clean_names() |>
  select( line:ada, -(staffing),-(staff_hours), -(exit_only) )

```


Convert the entry variable from character (YES vs NO) to a logical variable (the ifelse or case_match function may be useful).

```{r}
transit_tidy_df= 
  transit_df|>
  janitor::clean_names() |>
  select( line:ada, -(staffing),-(staff_hours), -(exit_only) ) |>
  mutate(
  entry = ifelse(entry =="YES", TRUE,FALSE))

```

Write a short paragraph about this dataset – explain briefly what variables the dataset contains, describe your data cleaning steps so far, and give the dimension (rows x columns) of the resulting dataset. Are these data tidy?

This data set provides data on transit routes, lines, and some spatial data. I have cleaned the names of some of these variables using the janitor function. I have also labeled missing values. I also changed the responses in the entry column to a logical vector of TRUE = "yes" and FALSE = "no" entry. After selecting certain columns, there are 1858 rows and 19 columns. This is not yet tidy data. Routes are separated into more than one column and is not easy to follow. Aside from that, the variable names are tidy.

How many distinct stations are there? Note that stations are identified both by name and by line (e.g. 125th St 8th Avenue; 125st Broadway; 125st Lenox); the distinct function may be useful here.

```{r}
transit_tidy_df |> 
  select(line, station_name)|>
  distinct()
```

There are 465 distinct station names considering the two identifiers of station name and line. 

How many stations are ADA compliant?
```{r}
transit_tidy_df |>
  filter(ada== TRUE) |>
  select(station_name, line)|>
  distinct()
```

The tibble I filtered for ada compliance equal to "TRUE" returned 468 rows suggesting 468 stations are ada compliant, but I needed to remove duplicate stations. There are 84 distinct stations which are ada compliant.

What proportion of station entrances or exits without vending allow entrance? 

- station entry = true and vending = NO (entry = "YES" ) 
- and station exit and vending = NO 
```{r}
transit_tidy_df|>
  filter(vending== "NO") |>
  pull(entry) |>
  mean()
```


The proportion of station entrances or exits without vending which allow entrance was 0.38. 

Reformat data so that route number and route name are distinct variables. 
- merge all route# columns 

```{r}
transit_tidy_route =
  transit_tidy_df |> 
  mutate(across(starts_with("route"), as.character))|>
  pivot_longer(
    cols= route1:route11, 
    names_to = "route_number", 
    names_prefix = "route",
    values_to = "route"
  )|>
  relocate(
    route_number, route,station_name
  )
```


How many distinct stations serve the A train? 
```{r}
transit_tidy_route |>
  filter(route =="A") |> 
  distinct(station_name, line)
```
There are 60 distinct station the A line serves.

Of the stations that serve the A train, how many are ADA compliant?

- ada compliant stations divided by distinct() stations that serve the == A train 
```{r}
transit_tidy_route |> 
  filter (route == "A" , ada== TRUE) |> 
  distinct(station_name, line)
```

Of those stations that serve the A train, there are 17 stations that are ada compliant. 

## Problem 2 

Read and clean the Mr. Trash Wheel excel sheet:

- specify the sheet in the Excel file and to omit non-data entries (rows with notes / figures; columns containing notes) using arguments in read_excel
- use reasonable variable names 
- omit rows that do not include dumpster-specific data
- round the number of sports balls to the nearest integer and converts the result to an integer variable (using as.integer)

```{r}
trash_df = 
  read_excel("202409 Trash Wheel Collection Data.xlsx", 
    sheet = "Mr. Trash Wheel",
    range = "A2:N586") |>
  janitor::clean_names()|>
  mutate(
    sports_balls= round(sports_balls,digit=1), 
    sports_balls = as.integer(sports_balls)
  )

```
Use a similar process to import, clean, and organize the data for Professor Trash Wheel and Gwynnda, and combine this with the Mr. Trash Wheel dataset to produce a single tidy dataset. To keep track of which Trash Wheel is which, you may need to add an additional variable to both datasets before combining.

Imported both datasets and added the same column which specifies which wheel the data is coming from.
```{r}
gwynnda_df = 
  read_excel("202409 Trash Wheel Collection Data.xlsx", 
    sheet = "Gwynnda Trash Wheel", 
    range = "A2:L157")|>
  janitor::clean_names()|>
  mutate(
    wheel= "gwynnda"
  )

professor_df = 
  read_excel("202409 Trash Wheel Collection Data.xlsx", 
    sheet = "Professor Trash Wheel", 
    range = "A2:M106") |>
  janitor::clean_names()|>
  mutate(
    wheel= "professor"
  )
trash_df = 
  read_excel("202409 Trash Wheel Collection Data.xlsx", 
    sheet = "Mr. Trash Wheel",
    range = "A2:N586") |>
  janitor::clean_names()|>
  mutate(
    sports_balls= round(sports_balls,digit=1), 
    sports_balls = as.integer(sports_balls), 
    wheel= "mr", 
    year= as.numeric(year)
  )

```

Merge the 3 datasets

- merge using the stack bind() method 

```{r}
wheel_df = 
  bind_rows(trash_df,gwynnda_df,professor_df)
```

Write a paragraph about these data; you are encouraged to use inline R. Be sure to note the number of observations in the resulting dataset, and give examples of key variables. For available data, what was the total weight of trash collected by Professor Trash Wheel? What was the total number of cigarette butts collected by Gwynnda in June of 2022?

```{r}
wheel_df |>
  filter(wheel == "professor") |>
  summarize(
     sum = sum(weight_tons)
  )
```


```{r}
wheel_df |>
  filter(month == "June", wheel == "gwynnda", year== "2022") |>
  summarize(
     sum = sum(cigarette_butts)
  )
```

There are `r nrow(wheel_df)` rows in the wheel_df which holds row entries from gwynnda `r nrow(gwynnda_df)` rows, professor `r nrow(professor_df)` rows, and mr. wheel datasets `r nrow(trash_df)` rows. Each row observation still shows observations from a date taken at a moment in time (year). Some key variables are `r colnames(wheel_df)` where "wheel" describes if the observations are from Mr.Trash wheel, Professor Trash Wheel, or Gwynnda Trash Wheel.
The total weight of trash collected by Professor Trash Wheel was 212.16 tons. 18,120 cigarette butts in June 2022 collected by Gwynnda Trash Wheel.


## Problem 3 

In the first part of this problem, your goal is to create a single, well-organized dataset with all the information contained in these data files. 

- import, clean, tidy, and otherwise wrangle each of these dataset
- check for completeness and correctness across datasets (e.g. by viewing individual datasets and using anti_join)

- merge to create a single, final dataset; and organize this so that variables and observations are in meaningful orders. 
- Export the result as a CSV in the directory containing the original datasets.

```{r}
bakers_df = 
  read_csv("gbb_datasets/bakers.csv", na= c("NA","",".")) |> 
  janitor:: clean_names() |>
  separate(
    baker_name, into = c("baker","last_name"), sep=" "
    ) |> 
  mutate(baker= str_to_lower(baker))

bakes_df = 
  read_csv("gbb_datasets/bakes.csv", na= c("NA","",".")) |> 
  janitor:: clean_names() |> 
  mutate(baker= str_to_lower(baker))

results_df = 
  read_csv("gbb_datasets/results.csv", na= c("NA","","."),
           skip = 2) |> 
  janitor:: clean_names() |> 
  mutate(baker = str_to_lower(baker))
  
```
Joining the first two datasets which have 3 matching identifiers.
```{r}
bakers_series = 
  full_join( results_df, bakes_df, by = c("baker", "episode", "series")) |> 
  relocate( baker, series, episode)

```
Joining the next dataset which only has 2 matching identifiers.
```{r}
bakers_full= 
  full_join (bakers_df,bakers_series, by = c("baker","series")) |>
  mutate(
    first_name = baker, 
    last_name = str_to_lower(last_name)
  )|> 
  relocate(first_name, last_name, series, episode)
```

```{r}
without_match_join2 = 
  anti_join(bakers_df,bakers_series, by = c("baker","series"))

without_match_join1 = 
  anti_join(results_df, bakes_df, by = c("baker", "episode", "series"))

```

```{r}
bakers_full |>
  distinct(first_name, last_name)

bakers_df |>
  distinct(baker, last_name)
```

After anti_joining some of these joinings for rows where x and y had no matches, I doubled checked by visually viewing some of these without-matches to ensure they are kept in the final dataset as long as they do not repeat an exisiting match with the same idenitfiers. I also ensured that the distinct names of bakers were also all present in the final dataset using the distinct function. I noticed there are 2 extra distinct names that was accumulated from the other datasets not included in the bakers_df. This could be due to a data entry error of who "jo" is since someone likely used a nick-name instead of their first and last name which requires further questioning.

I imported the datasets accounting for any missing data. I skipped 2 lines for the "results_df" since the first two rows were not column names. I cleaned the column names by using janitor and clean names functions. I also separated the bakers names from the "bakers_df" as it had both first and last names. This made it easier to combine these datasets by two identifiers series, episode, and name. I combined bakers_df by only bakers first name and series as it did not have a episode column. Since each dataset measured different observations in columns, I joined the datasets. I used full join to ensure that no episodes were lost from left or right joining when joining results_df and bakes_df. I then joined the bakers_df by baker and series to the dataset since it had no episode column. The final dataset has `r nrow(bakers_full)` rows and `r ncol(bakers_full)` columns which include `r colnames(bakers_full)`. 

Create a reader-friendly table showing the star baker or winner of each episode in Seasons 5 through 10. Comment on this table – were there any predictable overall winners? Any surprises?
- use knitr function 

```{r}
star_baker_df = 
  bakers_full |>
  filter(result== "STAR BAKER",series > 5)|>
  distinct(first_name, last_name, .keep_all=TRUE)|>
  relocate(first_name, last_name, series)|>
  select(first_name:episode, baker_age:technical, signature_bake, show_stopper)|>
  mutate(
    first_name= str_to_title(first_name),
    last_name = str_to_title(last_name)
  )|>
  knitr::kable(align = 'c')

star_baker_df
```
I was surprised to see a young star baker or winner be as young as 19 sine the majority of winners are older adults. The amount of technical a person had seemed to show if they would be winners. However, two individuals seemed to have had the highest technical and still won in a series.

Import, clean, tidy, and organize the viewership data in viewers.csv. Show the first 10 rows of this dataset. What was the average viewership in Season 1? In Season 5?

```{r}
viewers_df = 
  read_csv("gbb_datasets/viewers.csv", na= c("NA","",".")) |> 
  janitor:: clean_names()
```






